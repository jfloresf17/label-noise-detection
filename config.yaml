# Configuration for training Teacher and Student Model

# General configuration
device: "cuda"  # Use "cuda" for GPU or "cpu" for CPU

# Model parameters
learning_rate: 1e-4

# Data parameters
data:
  batch_size: 16
  num_workers: 16  
  whu_path: "/media/tidop/Datos_4TB/databases/whu"
  datacentric_image_path: "/media/tidop/Datos_4TB/databases/kaggle/dataset/training_patches"
  label_noisy_path: "/media/tidop/Datos_4TB/databases/kaggle/dataset/training_noisy_labels"

# Data transformations
Normalize: 
  apply: True
  WHU:
    # mean: [105.7880169 , 114.63701386, 112.91447218]  # Only training
    # std: [44.28023448, 41.85581294, 45.13255112]
    mean: [105.34253814, 114.2284708 , 112.52936415]  # Include test and validation
    std: [42.12451161, 39.52149692, 42.81161886]
  DataCentric:
    mean: [72.74413315, 99.76137101, 82.70024275]  
    std: [36.28290664, 34.82507359, 41.48902725]


# Knowledge distillation parameters
knowledge_distillation:
  alpha: 0.7  # Weight of distillation loss
  beta: 0.3  # Weight of supervised loss with noisy labels

# Training parameters
trainer:
  wandb_project: "UNet-Teacher-Training"  # Name of the Weights & Biases project
  experiment_name: "Student-Teacher Training"  # Name of the experiment
  max_epochs: 20  # Maximum number of epochs


